# SupportHub Golden Eval Dataset Blueprint (Data Task 9)

## Goal
Define a reproducible, machine-evaluable query set for objective RAG quality validation.

## Dataset Profile
- File: `evals/supporthub_golden_v1.jsonl`
- Size: 50 queries (within required 40-80 range)
- Language mix: CZ + EN
- Difficulty mix: basic + advanced

## Query Contract
Each row contains:
- `id`
- `query`
- `language`
- `difficulty`
- `expected_answer_type`
- `filters`
- `expected_source_types`
- `expected_sources`
- `tags`

## Coverage Targets
Query set covers:
- feature overview
- root cause
- incident timeline
- SLA analysis
- communication summary
- data quality/noise robustness
- decision and action planning

Evidence expectations include source types:
- issue
- journal
- attachment
- wiki
- news
- document
- file
- message

## Evaluation Loop
1. Validate dataset structure and coverage (`make eval`).
2. Run system on all golden queries and produce results JSONL.
3. Compute citation coverage, groundedness, retrieval hit rate.
4. Compare against baseline thresholds for regression gating.

## Reproducibility
- Dataset is generated by deterministic script: `scripts/eval/build_supporthub_golden.py`.
- No random sampling is used.
- Query IDs are stable and versioned (`supporthub_golden_v1`).
